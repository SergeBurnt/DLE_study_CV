# -*- coding: utf-8 -*-
"""Detection_VOCDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10wIxhO19lxs6ohf9AwHUXjd4DyBaKbMW
"""

# pip install --upgrade pip

# Commented out IPython magic to ensure Python compatibility.
# %pip install torchmetrics

import torch
import matplotlib.pyplot as plt
from torchvision.datasets import VOCDetection
from tqdm import tqdm
from torchvision.utils import draw_bounding_boxes
import torchvision.transforms as T
from torch.utils.data import DataLoader
import numpy as np
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn
from torchmetrics.detection.mean_ap import MeanAveragePrecision
from torchvision.ops import batched_nms
from IPython.display import clear_output
from torch.optim import Adam

path = "/home/jupyter/datasphere/project/data"

# dataset = VOCDetection(path, download=True)

dataset = VOCDetection(path, image_set='train')

dataset[0]

dataset[0][0]

target_names = []

for obj in tqdm(dataset):
    target_names.extend([x["name"] for x in obj[1]["annotation"]["object"]])

target_names = list(set(target_names))
target_names

target2index = {name: i for i, name in enumerate(target_names)}
index2target = {i: name for i, name in enumerate(target_names)}

def target_transform(voc_target):
    target = {}

    target["boxes"] = torch.tensor(
        [
            [
                float(x["bndbox"]["xmin"]),
                float(x["bndbox"]["ymin"]),
                float(x["bndbox"]["xmax"]),
                float(x["bndbox"]["ymax"])
            ] for x in voc_target["annotation"]["object"]
        ],
        dtype=torch.float32
    )

    target["labels"] = torch.tensor(
        [target2index[x["name"]] for x in voc_target["annotation"]["object"]],
        dtype=torch.int64
    )

    return target

def show(imgs):
    to_pil = T.ToPILImage()

    if not isinstance(imgs, list):
        imgs = [imgs]
    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(16, 8))

    for i, img in enumerate(imgs):
        img = img.detach()
        img = to_pil(img)
        axs[0, i].imshow(np.asarray(img))
        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])

def draw_boxes_single_image(image, boxes, labels):
    colors = ["red" for _ in boxes]
    show(draw_bounding_boxes(image, boxes, labels, colors=colors, width=3))

draw_boxes_single_image(
    T.PILToTensor()(dataset[0][0]),
    target_transform(dataset[0][1])["boxes"],
    [index2target[label.item()] for label in target_transform(dataset[0][1])["labels"]]
)

for i in range(10):
    draw_boxes_single_image(
        T.PILToTensor()(dataset[i][0]),
        target_transform(dataset[i][1])["boxes"],
        [index2target[label.item()] for label in target_transform(dataset[i][1])["labels"]]
    )

def collate_fn(batch):
    return tuple(zip(*batch))

dataset_train = VOCDetection(
    path,
    image_set="train",
    transform=T.ToTensor(),
    target_transform=target_transform
)

dataset_valid = VOCDetection(
    path,
    image_set="val",
    transform=T.ToTensor(),
    target_transform=target_transform
)

train_loader = DataLoader(dataset_train, batch_size=8, shuffle=True, num_workers=8, pin_memory=True, collate_fn=collate_fn)
valid_loader = DataLoader(dataset_valid,batch_size=8, shuffle=False, num_workers=8, pin_memory=True, collate_fn=collate_fn)

def get_detection_model():
    model = fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)
    model.roi_heads.box_predictor = FastRCNNPredictor(1024, len(target_names))
    return model

model = get_detection_model()
model

def apply_nms(orig_prediction, iou_thresh=0.3):

    keep = batched_nms(orig_prediction["boxes"], orig_prediction["scores"], orig_prediction["labels"], iou_thresh)
    final_prediction = orig_prediction

    final_prediction["boxes"] = final_prediction["boxes"][keep]
    final_prediction["scores"] = final_prediction["scores"][keep]
    final_prediction["labels"] = final_prediction["labels"][keep]

    return final_prediction

@torch.inference_mode()
def visualize(model, batch):
    model.eval()

    to_pil = T.ToPILImage()
    to_tensor = T.PILToTensor()
    xs, ys = batch

    for i, (x, y) in enumerate(zip(xs, ys)):
        prediction = {k: v.to(device) for k, v in model([x.to(device)])[0].items()}
        prediction = apply_nms(prediction)

        x = to_tensor(to_pil(x.cpu()))

        true_boxes = draw_bounding_boxes(
            x,
            boxes=y["boxes"],
            labels=[index2target[label.item()] for label in y["labels"]],
            colors="red",
            width=3
        )

        predicted_boxes = draw_bounding_boxes(
            x,
            boxes=prediction["boxes"],
            labels=[index2target[label.item()] for label in prediction["labels"]],
            colors="red",
            width=3
        )

        fig, axs = plt.subplots(1, 2, figsize=(16, 8), facecolor="white")

        axs[0].imshow(to_pil(true_boxes))
        axs[1].imshow(to_pil(predicted_boxes))

        axs[0].axis("off")
        axs[1].axis("off")

        axs[0].set_title("Target boxes")
        axs[1].set_title("Predicted boxes")

        plt.subplots_adjust(wspace=0, hspace=0.1)
        plt.show()

        if i >= 9:
            break

def train(model, train_loader):
    model.train()

    for x, y in tqdm(train_loader, desc="Train"):
        x = list(_.to(device).float() for _ in x)
        y = [{k: v.to(device) for k, v in t.items()} for t in y]
        optimizer.zero_grad()
        output = model(x, y)
        loss_sum = sum(loss for loss in output.values())
        loss_sum.backward()
        optimizer.step()

@torch.inference_mode()
def evaluate(model, loader):
    model.eval()

    metric = MeanAveragePrecision()

    for x, y in tqdm(loader, desc="Evaluation"):
        x = [_.to(device).float() for _ in x]
        output = model(x)
        output = [{k: v.cpu() for k, v in t.items()} for t in output]
        metric.update(output, y)

    return metric.compute()["map"]

def whole_train_valid_cycle(model, num_epochs, title):
    batch = next(iter(valid_loader))

    for epoch in range(num_epochs):
        train(model, train_loader)

        clear_output()

        visualize(model, batch)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)
model = get_detection_model().to(device)
optimizer = Adam([p for p in model.parameters() if p.requires_grad], lr=1e-4)

whole_train_valid_cycle(model, 15, "Faster R-CNN finetune")

visualize(model, next(iter(train_loader)))

visualize(model, next(iter(train_loader)))

train_map = evaluate(model, train_loader)
valid_map = evaluate(model, valid_loader)

print(f"Train mAP {train_map:.5f} Valid mAP {valid_map:.5f}")

